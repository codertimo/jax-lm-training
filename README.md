# JAX Language Model Training

> Currently this repo is under construction :) 

## todos

- [ ] writing corpus tokenizing and featurizing code with Apache Beam
- [ ] writing training code with single GPU
- [ ] writing evaluation code with single GPU
- [ ] writing metric tracking code with weight&bias
- [ ] train LM with single GPU and debug!
- [ ] writing parallelism code for multi-GPUs support.
- [ ] train LM with multi-GPUs and debug!
- [ ] training in TPU and TPU-Pod
- [ ] result comparison GPU, GPUs, TPU, TPU-Pod
- [ ] writing similar code with pytorch and compare the training performance
